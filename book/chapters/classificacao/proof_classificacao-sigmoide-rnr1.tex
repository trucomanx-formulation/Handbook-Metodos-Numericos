
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{myproofT}[Relativa ao Teorema \ref{theo:reglogrnr1:1}:]\label{proof:theo:reglogrnr1}
Dados
o vetor coluna $\VECTOR{x} \in \mathbb{R}^{N}$, os escalares $y \in \mathbb{R}$ e $c_m \in \mathbb{R}$,
uma função $f_{\VECTOR{c}}:\mathbb{R}^{N} \rightarrow \mathbb{R}$, 
uma função $h_{\VECTOR{c}}:\mathbb{R}^{N} \rightarrow \mathbb{R}$,  e 
definidas as seguintes equações,
\begin{equation}\label{eq:proof:theo:reglogrnr1:1}
y\equiv f_{\VECTOR{c}}(\VECTOR{x})= \frac{1}{1+e^{-h_{\VECTOR{c}}(\VECTOR{x}) }},
\quad h_{\VECTOR{c}}(\VECTOR{x}) =  \VECTOR{a}(\VECTOR{x})\VECTOR{c},
\end{equation}
ou seu equivalente: $logit(y)=h_{\VECTOR{c}}(\VECTOR{x})$,
onde $\VECTOR{c}=[c_1~ c_2~ ...~ c_m~ ...~ c_{N+1}]^{\transpose} \in \mathbb{R}^{N+1}$ é um vetor coluna e
$\VECTOR{a}(\VECTOR{x})=\begin{bmatrix} 
1& x_1& x_2& \hdots & x_m& \hdots& x_N
\end{bmatrix}$ é um vetor linha.
Se definimos um erro $e(\VECTOR{c})$ como
\begin{equation}\label{eq:proof:theo:reglogrnr1:2}
%e(\VECTOR{c}) = ||h(\VECTOR{x})-\VECTOR{y}||_{\MATRIX{W}}^2 \equiv \sum_{n=1}^{L} w_l||h(x_l)-y_l||^2,
e(\VECTOR{c}) =  \sum_{n=1}^{L} w_l||h_{\VECTOR{c}}(\VECTOR{x}_l)-logit(y_l)||^2,
\end{equation}
proveniente de avaliar $L$ amostras $\VECTOR{x}_l \in \mathbb{R}^{N}$ que pertencem a dois grupos, 
sendo que cada amostra $\VECTOR{x}_l$ tem uma etiqueta de grupo $y_l\in \{A,1-A\}$, 
onde $0<A\ll 0.5$ é escolhido por nós;
então, podemos rescrever a Eq. (\ref{eq:proof:theo:reglogrnr1:2}) como a Eq. (\ref{eq:proof:theo:reglogrnr1:3}),
\begin{equation}\label{eq:proof:theo:reglogrnr1:3}
e(\VECTOR{c}) \equiv ||\MATRIX{A}\VECTOR{c}-\VECTOR{z}||_{\MATRIX{W}}^2 
\end{equation}
onde os pesos $w_l \in \mathbb{R}_+$ são escolhidos por nós, e 
\begin{equation}\label{eq:proof:reglogrnr1:4}
\MATRIX{A}=\begin{bmatrix}
\VECTOR{a}(x_1)\\
\VECTOR{a}(x_2)\\
\vdots\\
\VECTOR{a}(x_l)\\
\vdots\\
\VECTOR{a}(x_L)\\
\end{bmatrix}
\equiv
\begin{bmatrix}
1 & \VECTOR{x}_1^{\transpose}\\
1 & \VECTOR{x}_2^{\transpose}\\
\vdots & \vdots\\
1 & \VECTOR{x}_l^{\transpose}\\
\vdots & \vdots\\
1 & \VECTOR{x}_L^{\transpose}\\ 
\end{bmatrix},
\quad
\VECTOR{z}=
\begin{bmatrix}
logit(y_1)  \\
logit(y_2)  \\
\vdots  \\
logit(y_l)  \\
\vdots \\
logit(y_L) \\
\end{bmatrix},
\quad
\MATRIX{W}=\funcdiag \left(
\begin{bmatrix}
w_1 \\
w_2 \\
\vdots  \\
w_l \\
\vdots \\
w_L \\
\end{bmatrix} \right).
\end{equation}


Usando o Teorema \ref{theo:minAxbCAxb}, sabemos que o vetor $\VECTOR{c}=\VECTOR{\hat{c}}$,
que minimiza a Eq. (\ref{eq:proof:theo:reglogrnr1:3}), pode ser obtido usando 
\begin{equation}\label{eq:proof:theo:reglogrnr1:5}
\VECTOR{\hat{c}}=[\MATRIX{A}^{\transpose}\MATRIX{W}\MATRIX{A}]^{-1}\MATRIX{A}^{\transpose}\MATRIX{W}\VECTOR{z}.
\end{equation}
\end{myproofT}

