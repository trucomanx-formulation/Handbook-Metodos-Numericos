\section{Provas dos teoremas}
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{myproofT}[Prova do Teorema \ref{theo:maphxr1r1}]\label{proof:theo:maphxr1r1}
Dados,
os escalares $x \in \mathbb{R}$, $y \in \mathbb{R}$ e $c_m \in \mathbb{R}$,
uma função polinomial $h_{\VECTOR{c}}:\mathbb{R} \rightarrow \mathbb{R}$, de grau $M$, e 
definida a seguinte equação,
\begin{equation}\label{eq:proof:theo:maphxr1r1:1}
y=h_{\VECTOR{c}}(x)\equiv \sum_{m=0}^{M}c_m x^m\equiv \VECTOR{a}(x)\VECTOR{c},
\end{equation}
onde $\VECTOR{c}=[c_1~ c_2~ ...~ c_m~ ...~ c_{M+1}]^{\transpose}$ é um vetor coluna e
$\VECTOR{a}(x)=\begin{bmatrix} 
1& x& x^2& \hdots & x^m& \hdots& x^M
\end{bmatrix}$ um vetor linha.
Se definimos um erro $e(\VECTOR{c})$ como
\begin{equation}\label{eq:proof:theo:maphxr1r1:2}
%e(\VECTOR{c}) = ||h(\VECTOR{x})-\VECTOR{y}||_{\MATRIX{W}}^2 \equiv \sum_{n=1}^{N} w_n||h(x_n)-y_n||^2,
e(\VECTOR{c}) =  \sum_{n=1}^{N} w_n||h_{\VECTOR{c}}(x_n)-y_n||^2,
\end{equation}
proveniente de avaliar $N$ amostras $x_n$ e $y_n$, 
que não cumprem necessariamente a Eq. (\ref{eq:proof:theo:maphxr1r1:1}), 
representadas pelos vetores $\VECTOR{x}=[x_1~ x_2~ ...~ x_n~ ...~ x_N]^{\transpose}$ e $\VECTOR{y}=[y_1~ y_2~ ...~ y_n~ ...~ y_N]^{\transpose}$,
ponderadas com os pesos $w_n$, representados pela matriz diagonal $\MATRIX{W}=\funcdiag([w_1~ w_2~ ...~ w_n~ ...~ w_N]^{\transpose})$.
Então, podemos rescrever a Eq. (\ref{eq:proof:theo:maphxr1r1:2}) como,
\begin{equation}\label{eq:proof:theo:maphxr1r1:3}
e(\VECTOR{c}) \equiv ||\MATRIX{A}\VECTOR{c}-\VECTOR{y}||_{\MATRIX{W}}^2 =  \sum_{n=1}^{N} w_n||\VECTOR{a}(x_n)\VECTOR{c}-y_n||^2,
\end{equation}
onde a matriz $\MATRIX{A}$ é definida como,
\begin{equation}\label{eq:proof:maphxr1r1:4}
\MATRIX{A}=\begin{bmatrix}
\VECTOR{a}(x_1)\\
\VECTOR{a}(x_2)\\
\vdots\\
\VECTOR{a}(x_n)\\
\vdots\\
\VECTOR{a}(x_N)\\
\end{bmatrix}.
\end{equation}


Usando o Teorema \ref{theo:minAxbCAxb}, sabemos que o vetor $\VECTOR{c}=\VECTOR{\hat{c}}$,
que minimiza a Eq. (\ref{eq:proof:theo:maphxr1r1:3}) pode ser achado usando 
\begin{equation}\label{eq:proof:theo:maphxr1r1:5}
\VECTOR{\hat{c}}=[\MATRIX{A}^{\transpose}\MATRIX{W}\MATRIX{A}]^{-1}\MATRIX{A}^{\transpose}\MATRIX{W}\VECTOR{y},
\end{equation}
\end{myproofT}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{myproofT}[Prova do Teorema \ref{theo:maphcxr1r1}]\label{proof:theo:maphcxr1r1}
Dados,
os escalares $x \in \mathbb{R}$ e $y \in \mathbb{R}$, o vetor coluna $\VECTOR{c} \in \mathbb{R}^M$, e 
definida a seguinte equação,
\begin{equation}\label{eq:proof:theo:maphcxr1r1:1}
y=h_{\VECTOR{c}}(x)\equiv h(\VECTOR{c},x),
\end{equation}
onde $\VECTOR{c}=[c_1~ c_2~ ...~ c_m~ ...~ c_M]^{\transpose}$ é um vetor coluna.
Se definimos um erro $e(\VECTOR{c})$ como
\begin{equation}\label{eq:proof:theo:maphcxr1r1:2}
e(\VECTOR{c}) =  \sum_{n=1}^{N} w_n||h(\VECTOR{c},x_n)-y_n||^2 + \alpha \sum_{m=1}^{M} d_n||c_m-c_{m,last}||^2,
\end{equation}
proveniente de:
\begin{itemize}
\item Avaliar $N$ amostras $x_n$ e $y_n$, que não cumprem necessariamente a Eq. (\ref{eq:proof:theo:maphcxr1r1:1}), 
\item Avaliar os $M$ coeficientes $c_m$ do vetor $\VECTOR{c}$ 
em contraposição de outros coeficientes $c_{m,last}$ do vetor $\VECTOR{c}_{last}$
que pode ser entendido como um ponto ao redor do qual é feita uma aproximação
linear de $h(\VECTOR{c},x)$ em relação a $\VECTOR{c}$; 
tudo isto ponderado com os pesos $d_m$.
\end{itemize}
Então podemos rescrever a Eq. (\ref{eq:proof:theo:maphcxr1r1:2}) como,
\begin{equation}\label{eq:proof:theo:maphcxr1r1:3}
e(\VECTOR{c}) =  ||\VECTOR{h}(\VECTOR{c})-\VECTOR{y}||_{\MATRIX{W}}^2 + \alpha||\VECTOR{c}-\VECTOR{c}_{last}||_{\MATRIX{D}}^2,
\end{equation}
onde $\alpha \in \mathbb{R}$ é um multiplicador de Lagrange escolhido por nós,
\begin{equation}
\VECTOR{h}(\VECTOR{c})=\begin{bmatrix}
h(\VECTOR{c},x_1)\\ 
h(\VECTOR{c},x_2)\\ 
%\vdots\\ 
%h(\VECTOR{c},x_n)\\ 
\vdots\\ 
h(\VECTOR{c},x_N)
\end{bmatrix},
~
%\VECTOR{x}=\begin{bmatrix}
%x_1\\ 
%x_2\\ 
%%\vdots\\ 
%%x_n\\ 
%\vdots\\ 
%x_N
%\end{bmatrix},
%~
\VECTOR{y}=\begin{bmatrix}
y_1\\ 
y_2\\ 
%\vdots\\ 
%y_n\\ 
\vdots\\ 
y_N
\end{bmatrix},
~
\MATRIX{W}=\funcdiag\left(\begin{bmatrix}
w_1\\ 
w_2\\ 
%\vdots\\ 
%w_n\\ 
\vdots\\ 
w_N
\end{bmatrix}\right),
~
\MATRIX{D}=\funcdiag\left(\begin{bmatrix}
d_1\\ 
d_2\\ 
%\vdots\\ 
%d_m\\ 
\vdots\\ 
d_M
\end{bmatrix}\right).
\end{equation}

Usando o Teorema \ref{theo:minfxbCfxb}, sabemos que o vetor $\VECTOR{c}$,
que minimiza a Eq. (\ref{eq:proof:theo:maphcxr1r1:3}) pode ser achado usando 
de forma iterativa a seguinte equação
\begin{equation}\label{eq:proof:theo:maphcxr1r1:5}
\VECTOR{c}_{i}=\VECTOR{c}_{i-1}-[\MATRIX{J}(\VECTOR{c}_{i-1})^{\transpose}\MATRIX{W}\MATRIX{J}(\VECTOR{c}_{i-1})+\alpha \MATRIX{D}]^{-1}\MATRIX{J}(\VECTOR{c}_{i-1})^{\transpose}\MATRIX{W}[\VECTOR{h}(\VECTOR{c}_{i-1})-\VECTOR{y}],
\end{equation}
onde a matriz $\MATRIX{J}(\VECTOR{c})$ 
$\equiv \frac{\partial \VECTOR{h}(\VECTOR{c})}{\partial \VECTOR{c}^{\transpose}}$ é a 
\hyperref[def:jacobian]{\textbf{matriz Jacobiana}}  de $\VECTOR{h}(\VECTOR{c})$,
\begin{equation}
\MATRIX{J}(\VECTOR{c})=\begin{bmatrix}
\VECTOR{j}(\VECTOR{c},x_1)\\ 
\VECTOR{j}(\VECTOR{c},x_2)\\ 
%\vdots\\ 
%\VECTOR{j}(\VECTOR{c},x_n)\\ 
\vdots\\ 
\VECTOR{j}(\VECTOR{c},x_N)
\end{bmatrix},
\quad
\begin{array}{lll}
\VECTOR{j}(\VECTOR{c},x) & = & \frac{\partial h(\VECTOR{c},x)}{\partial \VECTOR{c}^{\transpose}} \\
                       ~ & ~ & ~\\
                       ~ & = & \left[\frac{\partial h(\VECTOR{c},x)}{\partial c_1}\quad \frac{\partial h(\VECTOR{c},x)}{\partial c_2}\quad ...\quad \frac{\partial h(\VECTOR{c},x)}{\partial c_{m}} \quad ... \quad \frac{\partial h(\VECTOR{c},x)}{\partial c_{M}} \right]
\end{array}
\end{equation}
\end{myproofT}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{myproofT}[Prova do Teorema \ref{theo:maphxr2r1}]\label{proof:theo:maphxr2r1}
Dados,
os escalares $x \in \mathbb{R}$, $y \in \mathbb{R}$, $z \in \mathbb{R}$ e $c_m \in \mathbb{R}$,
uma função polinomial $h_{\VECTOR{c}}:\mathbb{R}^2 \rightarrow \mathbb{R}$, de grau $M$, e 
definida a seguinte equação,
\begin{equation}\label{eq:proof:theo:maphxr2r1:1}
\begin{matrix}
z=h_{\VECTOR{c}}(x,y) & \equiv & +c_{1}\\
              ~ & ~ & +c_{2}~x + c_{3}~y\\
              ~ & ~ & +c_{4}~x^2 +c_{5}~xy + c_{6}~y^2\\
              ~ & ~ &  ...\\
              ~ & ~ & +\sum \limits_{l=0}^{M}c_{\left\{ \frac{M(M+1)}{2}+l+1\right\}}~x^{M-l}y^{l};
\end{matrix} 
\end{equation}
que também pode ser escrita como
\begin{equation}
z=\VECTOR{a}(x,y)\VECTOR{c},
\end{equation}
onde $\VECTOR{c}=[c_1~ c_2~ ...~ c_m~ ...~ c_{\frac{(M+1)(M+2)}{2}}]^{\transpose} \in \mathbb{R}^{\frac{(M+1)(M+2)}{2}}$ é um vetor coluna e
$\VECTOR{a}(x,y)$ um vetor linha,
\begin{equation}
\VECTOR{a}(x,y)= 
\begin{bmatrix}
\VECTOR{s}_{0}(x,y) & \VECTOR{s}_{1}(x,y) &  \dots  & \VECTOR{s}_{M}(x,y)
\end{bmatrix},
\end{equation}
\begin{equation}
\VECTOR{s}_{m}(x,y)=
\begin{bmatrix}
x^m  & x^{m-1} y  & x^{m-2} y^2    & \dots  & x y^{m-1} &  y^m 
\end{bmatrix},
\qquad
\VECTOR{s}_{0}(x,y)=1.
\end{equation}
Se definimos um erro $e(\VECTOR{c})$ como
\begin{equation}\label{eq:proof:theo:maphxr2r1:2}
%e(\VECTOR{c}) = ||h(\VECTOR{x})-\VECTOR{y}||_{\MATRIX{W}}^2 \equiv \sum_{n=1}^{N} w_n||h(x_n)-y_n||^2,
e(\VECTOR{c}) 
=  
\sum_{n=1}^{N} w_n||h_{\VECTOR{c}}(x_n,y_n)-z_n||^2 
%\equiv 
%||\VECTOR{h}(\VECTOR{x},\VECTOR{y})-\VECTOR{z}||^2_{\MATRIX{W}},
\end{equation}
proveniente de avaliar $N$ amostras $x_n$ e $y_n$, 
que não cumprem necessariamente a Eq. (\ref{eq:proof:theo:maphxr2r1:1}), 
representadas pelos vetores 
$\VECTOR{x}=[x_1~ x_2~ ...~ x_n~ ...~ x_N]^{\transpose}$,
$\VECTOR{y}=[y_1~ y_2~ ...~ y_n~ ...~ y_N]^{\transpose}$ e 
$\VECTOR{z}=[z_1~ z_2~ ...~ z_n~ ...~ z_N]^{\transpose}$,
ponderadas com os pesos $w_n$, representados pela matriz diagonal $\MATRIX{W}=\funcdiag([w_1~ w_2~ ...~ w_n~ ...~ w_N]^{\transpose})$.
Então, podemos rescrever a Eq. (\ref{eq:proof:theo:maphxr2r1:2}) como,
\begin{equation}\label{eq:proof:theo:maphxr2r1:3}
e(\VECTOR{c}) \equiv ||\MATRIX{A}\VECTOR{c}-\VECTOR{z}||_{\MATRIX{W}}^2 =  \sum_{n=1}^{N} w_n||\VECTOR{a}(x_n,y_n)\VECTOR{c}-z_n||^2,
\end{equation}
onde a matriz $\MATRIX{A}$ é definida como,
\begin{equation}\label{eq:proof:maphxr2r1:4}
\MATRIX{A}\equiv \MATRIX{A}(\VECTOR{x},\VECTOR{y})=\begin{bmatrix}
\VECTOR{a}(x_1,y_1)\\
\VECTOR{a}(x_2,y_2)\\
\vdots\\
\VECTOR{a}(x_n,y_n)\\
\vdots\\
\VECTOR{a}(x_N,y_N)
\end{bmatrix}.
\end{equation}


Usando o Teorema \ref{theo:minAxbCAxb}, sabemos que o vetor $\VECTOR{c}=\VECTOR{\hat{c}}$,
que minimiza a Eq. (\ref{eq:proof:theo:maphxr2r1:3}) pode ser achado usando 
\begin{equation}\label{eq:proof:theo:maphxr2r1:5}
\VECTOR{\hat{c}}=[\MATRIX{A}^{\transpose}\MATRIX{W}\MATRIX{A}]^{-1}\MATRIX{A}^{\transpose}\MATRIX{W}\VECTOR{z},
\end{equation}
\end{myproofT}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{myproofT}[Prova do Teorema \ref{theo:maphcxrnr1}]\label{proof:theo:maphcxrnr1}
Dados,
o escalar $z \in \mathbb{R}$, os vetores coluna $\VECTOR{x} \in \mathbb{R}^N$ e $\VECTOR{c} \in \mathbb{R}^M$, e 
definida a seguinte equação,
\begin{equation}\label{eq:proof:theo:maphcxrnr1:1}
z=h_{\VECTOR{c}}(\VECTOR{x})\equiv h(\VECTOR{c},\VECTOR{x}),
\end{equation}
onde $\VECTOR{c}=[c_0~ c_1~ ...~ c_m~ ...~ c_M]^{\transpose}$ é um vetor coluna.
Se definimos um erro $e(\VECTOR{c})$ como
\begin{equation}\label{eq:proof:theo:maphcxrnr1:2}
e(\VECTOR{c}) =  \sum_{n=1}^{N} w_n||h(\VECTOR{c},\VECTOR{x}_n)-z_n||^2 + \alpha \sum_{m=1}^{M} d_n||c_m-c_{m,last}||^2,
\end{equation}
proveniente de:
\begin{itemize}
\item Avaliar $N$ pontos $\VECTOR{x}_n$ e $z_n$, que não cumprem necessariamente a Eq. (\ref{eq:proof:theo:maphcxrnr1:1}), 
\item Avaliar os $M$ coeficientes $c_m$ do vetor $\VECTOR{c}$ 
em contraposição de outros coeficientes $c_{m,last}$ do vetor $\VECTOR{c}_{last}$
que pode ser entendido como um ponto ao redor do qual é feita uma aproximação
linear de $h(\VECTOR{c},\VECTOR{x})$ em relação a $\VECTOR{c}$; 
tudo isto ponderado com os pesos $d_m$.
\end{itemize}
Então podemos rescrever a Eq. (\ref{eq:proof:theo:maphcxrnr1:2}) como,
\begin{equation}\label{eq:proof:theo:maphcxrnr1:3}
e(\VECTOR{c}) =  ||\VECTOR{h}(\VECTOR{c})-\VECTOR{z}||_{\MATRIX{W}}^2 + \alpha||\VECTOR{c}-\VECTOR{c}_{last}||_{\MATRIX{D}}^2,
\end{equation}
onde $\alpha \in \mathbb{R}$ é um multiplicador de Lagrange escolhido por nós,
\begin{equation}
\VECTOR{h}(\VECTOR{c})=\begin{bmatrix}
h(\VECTOR{c},\VECTOR{x}_1)\\ 
h(\VECTOR{c},\VECTOR{x}_2)\\ 
%\vdots\\ 
%h(\VECTOR{c},\VECTOR{x}_n)\\ 
\vdots\\ 
h(\VECTOR{c},\VECTOR{x}_N)
\end{bmatrix},
~
\VECTOR{z}=\begin{bmatrix}
z_1\\ 
z_2\\ 
%\vdots\\ 
%z_n\\ 
\vdots\\ 
z_N
\end{bmatrix},
~
\MATRIX{W}=\funcdiag\left(\begin{bmatrix}
w_1\\ 
w_2\\ 
%\vdots\\ 
%w_n\\ 
\vdots\\ 
w_N
\end{bmatrix}\right),
~
\MATRIX{D}=\funcdiag\left(\begin{bmatrix}
d_1\\ 
d_2\\ 
%\vdots\\ 
%d_m\\ 
\vdots\\ 
d_M
\end{bmatrix}\right).
\end{equation}

Usando o Teorema \ref{theo:minfxbCfxb}, sabemos que o vetor $\VECTOR{c}$,
que minimiza a Eq. (\ref{eq:proof:theo:maphcxrnr1:3}) pode ser achado usando 
de forma iterativa a seguinte equação
\begin{equation}\label{eq:proof:theo:maphcxrnr1:5}
\VECTOR{c}_{i}=\VECTOR{c}_{i-1}-[\MATRIX{J}(\VECTOR{c}_{i-1})^{\transpose}\MATRIX{W}\MATRIX{J}(\VECTOR{c}_{i-1})+\alpha \MATRIX{D}]^{-1}\MATRIX{J}(\VECTOR{c}_{i-1})^{\transpose}\MATRIX{W}[\VECTOR{h}(\VECTOR{c}_{i-1})-\VECTOR{z}],
\end{equation}
onde a matriz $\MATRIX{J}(\VECTOR{c})$ 
$\equiv \frac{\partial \VECTOR{h}(\VECTOR{c})}{\partial \VECTOR{c}^{\transpose}}$ é a 
\hyperref[def:jacobian]{\textbf{matriz Jacobiana}}  de $\VECTOR{h}(\VECTOR{c})$,
\begin{equation}
\MATRIX{J}(\VECTOR{c})=\begin{bmatrix}
\VECTOR{j}(\VECTOR{c},\VECTOR{x}_1)\\ 
\VECTOR{j}(\VECTOR{c},\VECTOR{x}_2)\\ 
%\vdots\\ 
%\VECTOR{j}(\VECTOR{c},\VECTOR{x}_n)\\ 
\vdots\\ 
\VECTOR{j}(\VECTOR{c},\VECTOR{x}_N)
\end{bmatrix},
\quad
\begin{array}{lll}
\VECTOR{j}(\VECTOR{c},\VECTOR{x}) & = & \frac{\partial h(\VECTOR{c},\VECTOR{x})}{\partial \VECTOR{c}^{\transpose}} \\
                       ~ & ~ & ~\\
                       ~ & = & \left[\frac{\partial h(\VECTOR{c},\VECTOR{x})}{\partial c_1}\quad \frac{\partial h(\VECTOR{c},\VECTOR{x})}{\partial c_2}\quad ...\quad \frac{\partial h(\VECTOR{c},\VECTOR{x})}{\partial c_{m}} \quad ... \quad \frac{\partial h(\VECTOR{c},\VECTOR{x})}{\partial c_{M}} \right]
\end{array}
\end{equation}
\end{myproofT}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{myproofT}[Prova do Teorema \ref{theo:maphxr2r2}]\label{proof:theo:maphxr2r2}
Dados,
os escalares $x \in \mathbb{R}$, $y \in \mathbb{R}$, $u \in \mathbb{R}$, $v \in \mathbb{R}$ e 
os vetores $\VECTOR{c} \in \mathbb{R}^{\frac{(M+1)(M+2)}{2}}$ e $\VECTOR{d} \in \mathbb{R}^{\frac{(M+1)(M+2)}{2}}$,
uma função polinomial $\VECTOR{h}:\mathbb{R}^2 \rightarrow \mathbb{R}^2$, de grau $M$, e 
definida a seguintes equações,
\begin{equation}\label{eq:proof:theo:maphxr2r2:0}
\begin{bmatrix}
u\\
v
\end{bmatrix}=
\VECTOR{h}(x,y)\equiv
\begin{bmatrix}
h_{\VECTOR{c}}(x,y)\\
h_{\VECTOR{d}}(x,y)
\end{bmatrix}
\end{equation}
\begin{equation}\label{eq:proof:theo:maphxr2r2:1}
\begin{matrix}
h_{\VECTOR{c}}(x,y) & \equiv & +c_{1}\\
              ~ & ~ & +c_{2}~x + c_{3}~y\\
              ~ & ~ & +c_{4}~x^2 +c_{5}~xy + c_{6}~y^2\\
              ~ & ~ &  ...\\
              ~ & ~ & +\sum \limits_{l=0}^{M}c_{\left\{ \frac{M(M+1)}{2}+l+1\right\}}~x^{M-l}y^{l};
\end{matrix}
\begin{matrix}
h_{\VECTOR{d}}(x,y) & \equiv & +d_{1}\\
              ~ & ~ & +d_{2}~x + d_{3}~y\\
              ~ & ~ & +d_{4}~x^2 +d_{5}~xy + d_{6}~y^2\\
              ~ & ~ &  ...\\
              ~ & ~ & +\sum \limits_{l=0}^{M}d_{\left\{ \frac{M(M+1)}{2}+l+1\right\}}~x^{M-l}y^{l};
\end{matrix}  
\end{equation}
que também podem ser escrita como
\begin{equation}
u=\VECTOR{a}(x,y)\VECTOR{c},
\qquad
v=\VECTOR{a}(x,y)\VECTOR{d},
\end{equation}
onde 
$\VECTOR{c}=[c_1~ c_2~ ...~ c_m~ ...~ c_{\frac{(M+1)(M+2)}{2}}]^{\transpose} \in \mathbb{R}^{\frac{(M+1)(M+2)}{2}}$ e
$\VECTOR{d}=[d_1~ d_2~ ...~ d_m~ ...~ d_{\frac{(M+1)(M+2)}{2}}]^{\transpose} \in \mathbb{R}^{\frac{(M+1)(M+2)}{2}}$ 
são vetores coluna, e
$\VECTOR{a}(x,y)$ é um vetor linha,
\begin{equation}
\VECTOR{a}(x,y)= 
\begin{bmatrix}
\VECTOR{s}_{0}(x,y) & \VECTOR{s}_{1}(x,y) &  \dots  & \VECTOR{s}_{M}(x,y)
\end{bmatrix},
\end{equation}
\begin{equation}
\VECTOR{s}_{m}(x,y)=
\begin{bmatrix}
x^m  & x^{m-1} y  & x^{m-2} y^2    & \dots  & x y^{m-1} &  y^m 
\end{bmatrix},
\qquad
\VECTOR{s}_{0}(x,y)=1.
\end{equation}
Se definimos um erro $e_{\VECTOR{c}}(\VECTOR{c})$ e $e_{\VECTOR{d}}(\VECTOR{d})$ como
\begin{equation}\label{eq:proof:theo:maphxr2r2:2}
%e(\VECTOR{c}) = ||h(\VECTOR{x})-\VECTOR{y}||_{\MATRIX{W}}^2 \equiv \sum_{n=1}^{N} w_n||h(x_n)-y_n||^2,
e_{\VECTOR{c}}(\VECTOR{c}) 
=  
\sum_{n=1}^{N} w_n||h_{\VECTOR{c}}(x_n,y_n)-u_n||^2,
\qquad
e_{\VECTOR{d}}(\VECTOR{d}) 
=  
\sum_{n=1}^{N} w_n||h_{\VECTOR{d}}(x_n,y_n)-v_n||^2 
\end{equation}
proveniente de avaliar $N$ amostras $\{x_n,y_n\}$ e $\{u_n,v_n\}$, 
que não cumprem necessariamente a Eq. (\ref{eq:proof:theo:maphxr2r2:1}), 
representadas pelos vetores 
$\VECTOR{x}=[x_1~ x_2~ ...~ x_n~ ...~ x_N]^{\transpose}$,
$\VECTOR{y}=[y_1~ y_2~ ...~ y_n~ ...~ y_N]^{\transpose}$, 
$\VECTOR{u}=[u_1~ u_2~ ...~ u_n~ ...~ u_N]^{\transpose}$ e
$\VECTOR{v}=[v_1~ v_2~ ...~ v_n~ ...~ v_N]^{\transpose}$,
ponderadas com os pesos $w_n$, representados pela matriz diagonal $\MATRIX{W}=\funcdiag([w_1~ w_2~ ...~ w_n~ ...~ w_N]^{\transpose})$.
Então, podemos rescrever a Eq. (\ref{eq:proof:theo:maphxr2r2:2}) como,
\begin{equation}\label{eq:proof:theo:maphxr2r2:3}
e_{\VECTOR{c}}(\VECTOR{c}) = ||\MATRIX{A}\VECTOR{c}-\VECTOR{u}||_{\MATRIX{W}}^2 
\qquad
e_{\VECTOR{d}}(\VECTOR{d}) = ||\MATRIX{A}\VECTOR{d}-\VECTOR{v}||_{\MATRIX{W}}^2 
\end{equation}
onde a matriz $\MATRIX{A}$ é definida como,
\begin{equation}\label{eq:proof:maphxr2r2:4}
\MATRIX{A}\equiv \MATRIX{A}(\VECTOR{x},\VECTOR{y})=\begin{bmatrix}
\VECTOR{a}(x_1,y_1)\\
\VECTOR{a}(x_2,y_2)\\
\vdots\\
\VECTOR{a}(x_n,y_n)\\
\vdots\\
\VECTOR{a}(x_N,y_N)
\end{bmatrix}.
\end{equation}


Usando o Teorema \ref{theo:minAxbCAxb}, sabemos que os vetores $\VECTOR{c}=\VECTOR{\hat{c}}$ e $\VECTOR{d}=\VECTOR{\hat{d}}$,
que minimizam a Eq. (\ref{eq:proof:theo:maphxr2r2:3}) podem ser achados usandos 
\begin{equation}\label{eq:proof:theo:maphxr2r2:5}
\VECTOR{\hat{c}}=[\MATRIX{A}^{\transpose}\MATRIX{W}\MATRIX{A}]^{-1}\MATRIX{A}^{\transpose}\MATRIX{W}\VECTOR{u},
\qquad
\VECTOR{\hat{d}}=[\MATRIX{A}^{\transpose}\MATRIX{W}\MATRIX{A}]^{-1}\MATRIX{A}^{\transpose}\MATRIX{W}\VECTOR{v},
\end{equation}
\end{myproofT}
