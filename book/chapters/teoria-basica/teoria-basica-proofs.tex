\section{Provas dos teoremas}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{myproofT}[Prova do Teorema \ref{theo:simetricmatrix0}:]\label{proof:theo:simetricmatrix0}
Conhecida uma matriz quadrada $\MATRIX{A} \in \mathbb{R}^{N \times N}$,
Se definimos a matriz simétrica $\MATRIX{S}=\frac{\MATRIX{A}+\MATRIX{A}^{\transpose}}{2}$ e
o vetor $\VECTOR{x} \in \mathbb{R}^{N}$, então
\begin{equation}
\VECTOR{x}^{\transpose}\MATRIX{S}\VECTOR{x} = 
\frac{\VECTOR{x}^{\transpose}\MATRIX{A}\VECTOR{x}}{2} +
\frac{\VECTOR{x}^{\transpose}\MATRIX{A}^{\transpose}\VECTOR{x}}{2}
\end{equation}
dado que cada somando da equação anterior é um escalar podemos afirmar que 
\begin{equation}
\VECTOR{x}^{\transpose}\MATRIX{S}\VECTOR{x} = 
\frac{\VECTOR{x}^{\transpose}\MATRIX{A}\VECTOR{x}}{2} +
\left[\frac{\VECTOR{x}^{\transpose}\MATRIX{A}^{\transpose}\VECTOR{x}}{2}\right]^{\transpose}
\end{equation}
e consequentemente que
\begin{equation}
\VECTOR{x}^{\transpose}\MATRIX{S}\VECTOR{x} = 
\VECTOR{x}^{\transpose}\MATRIX{A}\VECTOR{x} =
\VECTOR{x}^{\transpose}\MATRIX{A}^{\transpose}\VECTOR{x}
\end{equation} 
\end{myproofT}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{myproofT}[Prova do Teorema \ref{theo:matrixgeneric3}:]\label{proof:theo:matrixgeneric3}
Conhecida uma matriz quadrada $\MATRIX{A} \in \mathbb{R}^{N \times N}$, 
os  autovalores $\lambda_n$, $\forall n \in \{1, 2, ..., N\}$ são calculados mediante 
seu polinômio caraterístico,
\begin{equation}
p(\lambda)=det(\MATRIX{A}-\lambda \MATRIX{I})
\end{equation}
onde $\MATRIX{I}$ é uma matriz identidade.
\begin{equation}
p(\lambda)^{\transpose}=det(\MATRIX{A}-\lambda \MATRIX{I})^{\transpose}
\end{equation}
\begin{equation}
p(\lambda)=det(\MATRIX{A}^{\transpose}-\lambda \MATRIX{I}^{\transpose})
\end{equation}
\begin{equation}
p(\lambda)=det(\MATRIX{A}^{\transpose}-\lambda \MATRIX{I})
\end{equation}
\end{myproofT}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{myproofT}[Prova do Teorema \ref{theo:simetricmatrix4}:]\label{proof:theo:simetricmatrix4}
Conhecida uma matriz quadrada $\MATRIX{A} \in \mathbb{R}^{N \times N}$ não singular, 
\begin{equation}
\MATRIX{I}=\MATRIX{A}\MATRIX{A}^{-1}
\end{equation}
\begin{equation}
\MATRIX{I}^{\transpose}=\left\{\MATRIX{A}\MATRIX{A}^{-1}\right\}^{\transpose}
\end{equation}
\begin{equation}
\MATRIX{I}=\left\{\MATRIX{A}^{-1}\right\}^{\transpose} \MATRIX{A}^{\transpose}
\end{equation}
\begin{equation}
\MATRIX{I}=\left\{\MATRIX{A}^{-1}\right\}^{\transpose} \MATRIX{A}
\end{equation}
\begin{equation}
\MATRIX{A}^{-1}=\left\{\MATRIX{A}^{-1}\right\}^{\transpose} 
\end{equation}
\end{myproofT}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{myproofT}[Prova do Teorema \ref{theo:semipositivematrix1}:]\label{proof:theo:semipositivematrix1}
Conhecida uma matriz $\MATRIX{A} \in \mathbb{R}^{N \times M}$,
e um vetor $\VECTOR{x} \in \mathbb{R}^{N}$ não nulo,
\begin{equation}
\VECTOR{x}^{\transpose}\MATRIX{A}\MATRIX{A}^{\transpose}\VECTOR{x}
\end{equation}
\begin{equation}
\left\{\MATRIX{A}^{\transpose}\VECTOR{x}\right\}\left\{\MATRIX{A}^{\transpose}\VECTOR{x}\right\}
\end{equation}
se $\VECTOR{y}=\MATRIX{A}^{\transpose}\VECTOR{x}$, 
podemos percebe que existe a posibilidade de que $\VECTOR{y}=\VECTOR{0}$ seja nulo,
pelo que podemos afirmar que 
\begin{equation}
\VECTOR{y}^{\transpose}\VECTOR{y}\geq 0
\end{equation}
\begin{equation}
\VECTOR{x}^{\transpose}\MATRIX{A}\MATRIX{A}^{\transpose}\VECTOR{x}
\end{equation}
\end{myproofT}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{myproofT}[Prova do Teorema \ref{theo:semipositivematrix1}:]\label{proof:theo:semipositivematrix2}
Conhecida uma matriz $\MATRIX{A} \in \mathbb{R}^{N \times N}$, com  autovalores $\lambda_n$,
e autovetores $\VECTOR{v}_n$, $\forall n \in \{1, 2, ..., N\}$.
\begin{equation}
\MATRIX{A}\VECTOR{v}_n = \lambda_n \VECTOR{v}_n, 
\end{equation}
\begin{equation}
\VECTOR{v}_n^{\transpose}\MATRIX{A}\VECTOR{v}_n = \lambda_n \VECTOR{v}_n^{\transpose}\VECTOR{v}_n;
\end{equation}
sabendo que a matriz é semidefinida positiva,
\begin{equation}
0 \leq \VECTOR{v}^{\transpose}\MATRIX{A}\VECTOR{v} = \lambda_n \VECTOR{v}_n^{\transpose}\VECTOR{v}_n 
\end{equation}
\begin{equation}
\lambda_n \VECTOR{v}_n^{\transpose}\VECTOR{v}_n \geq 0,
\end{equation}
dado que sempre $\VECTOR{v}_n^{\transpose}\VECTOR{v}_n> 0$, concluimos
\begin{equation}
\lambda_n  \geq 0,
\end{equation} 
\end{myproofT}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{myproofT}[Prova do Teorema \ref{theo:positivematrix1}:]\label{proof:theo:positivematrix1}
Conhecida uma matriz quadrada $\MATRIX{A} \in \mathbb{R}^{N \times N}$ e definida positiva, com  autovalores $\lambda_n$,
e autovetores $\VECTOR{e}_n$, $\forall n \in \{1, 2, ..., N\}$, de modo que,
\begin{equation}\label{eq:proof:theo:positivematrix1:0}
\MATRIX{A}\VECTOR{e}_n=\lambda_n \VECTOR{e}_n.
\end{equation}
\begin{equation}\label{eq:proof:theo:positivematrix1:1}
\VECTOR{e}_n^{\transpose}\MATRIX{A}\VECTOR{e}_n=\lambda_n \VECTOR{e}_n^{\transpose}\VECTOR{e}_n.
\end{equation}

\begin{itemize}
\item Assim, usando a Eq. (\ref{eq:proof:theo:positivematrix1:1}) e a Definição \ref{def:positivematrix0} podemos afirmar que
\begin{equation}
\VECTOR{e}_n^{\transpose}\MATRIX{A}\VECTOR{e}_n >0
\end{equation} 
o que implica que
\begin{equation}
\lambda_n \VECTOR{e}_n^{\transpose}\VECTOR{e}_n > 0
\quad \rightarrow \quad
\lambda_n  \geq 0
\end{equation} 
\end{itemize}
\end{myproofT}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{myproofT}[Prova do Teorema \ref{theo:positivematrix1}:]\label{proof:theo:positivematrix2}
Conhecida uma matriz quadrada $\MATRIX{A} \in \mathbb{R}^{N \times N}$ simetrica,
com  autovalores $\lambda_n>0$, $\forall n \in \{1, 2, ..., N\}$;
sabemos pelo Teorema \ref{theo:simetricmatrix2}, 
que existe uma \hyperref[def:ortogonalmatrix0]{\textbf{matriz ortogonal}} $\MATRIX{Q}$,
\begin{equation}
\MATRIX{Q}^{\transpose}\MATRIX{A}\MATRIX{Q} = \MATRIX{\lambda}_{\MATRIX{A}}\equiv
\begin{bmatrix}
\lambda_1 & 0         & ...    & 0 \\
0         & \lambda_2 & ...    & 0 \\
\vdots    & \vdots    & \ddots & \vdots \\
0         & 0         & ...    & \lambda_N
\end{bmatrix}.
\end{equation}
Se multiplicamos esta igualdade por um vector $\VECTOR{x} \in \mathbb{R}^N$ (non-zero), obtemos,
\begin{equation}
\VECTOR{x}^{\transpose} \MATRIX{Q}^{\transpose}\MATRIX{A}\MATRIX{Q} \VECTOR{x} = 
\VECTOR{x}^{\transpose} \MATRIX{\lambda}_{\MATRIX{A}} \VECTOR{x}
\end{equation}
\begin{equation}
\VECTOR{x}^{\transpose} \MATRIX{Q}^{\transpose}\MATRIX{A}\MATRIX{Q} \VECTOR{x} 
= \sum \limits_{n=1}^{N}\lambda_n x_n^2.
\end{equation}
Aplicando uma troca de variaveis de modo que $\VECTOR{y}=\MATRIX{Q} \VECTOR{x}$,
\begin{equation}
\VECTOR{y}^{\transpose} \MATRIX{A} \VECTOR{y} 
= \sum \limits_{n=1}^{N}\lambda_n x_n^2.
\end{equation}
Dado que a soma de todos os $\lambda_n x_n^2$ é positiva, podemos afirmar que
\begin{equation}
\VECTOR{y}^{\transpose} \MATRIX{A} \VECTOR{y} > 0;
\end{equation}
é dizer $\MATRIX{A}$ é definida positiva.
\end{myproofT}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{myproofT}[Prova do Teorema \ref{theo:positivematrix:2}:]\label{proof:theo:positivematrix:2}
Conhecida uma matriz definida positiva e simétrica $\MATRIX{A} \in \mathbb{R}^{N \times N}$,
com  autovalores $\lambda_n>0$, $\forall n \in \{1, 2, ..., N\}$;
se $\MATRIX{A}$  é invertível então os autovalores de $\MATRIX{A}^{-1}$ são  $\frac{1}{\lambda_n}>0$.
Dado que $\MATRIX{A}$ é simétrica, $\MATRIX{A}^{-1}$ é simétrica também;
então se $\MATRIX{A}^{-1}$ é simétrica com autovalores positivos, esta é definida positiva.
\end{myproofT}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{myproofT}[Prova do Teorema \ref{theo:similhante1}:]\label{proof:theo:similhante1}
Conhecidas as matrizes semelhantes $\MATRIX{A} \in \mathbb{C}^{N \times N}$ e $\MATRIX{B} \in \mathbb{R}^{N \times N}$,
de modo que se cumpre que 
\begin{equation}
\MATRIX{A} = \MATRIX{P}^{-1} \MATRIX{B} \MATRIX{P}.
\end{equation}
para um $\MATRIX{P}$ dado.

Se calculamos o polinômio carateristico $p_{\MATRIX{A}}(\lambda)$ de $\MATRIX{A}$,
\begin{equation}
p_{\MATRIX{A}}(\lambda)=det\left(A-\lambda \MATRIX{I}\right),
\end{equation}
podemos rescrever esta equação
\begin{equation}
p_{\MATRIX{A}}(\lambda)=det\left(\MATRIX{P}^{-1} \MATRIX{B} \MATRIX{P}-\lambda \MATRIX{P}^{-1} \MATRIX{I} \MATRIX{P} \right),
\end{equation}
\begin{equation}
p_{\MATRIX{A}}(\lambda)=det\left( \MATRIX{P}^{-1}\right)det\left( \MATRIX{B} -\lambda \MATRIX{I} \right)det\left( \MATRIX{P}\right),
\end{equation}
\begin{equation}
p_{\MATRIX{A}}(\lambda)=det\left( \MATRIX{B} -\lambda \MATRIX{I} \right) = p_{\MATRIX{B}}(\lambda),
\end{equation}
\end{myproofT}
