
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{myproofT}[Relativa ao Teorema \ref{theo:reglogrnr1nolinear:1}]\label{proof:theo:reglogrnr1nolinear}
Dados,
o vetor coluna $\VECTOR{x} \in \mathbb{R}^{N}$, os escalares $y \in \mathbb{R}$ e $c_m \in \mathbb{R}$,
as funções $f_{\VECTOR{c}}:\mathbb{R}^{N} \rightarrow \mathbb{R}$, 
e $h_{\VECTOR{c}}:\mathbb{R}^{N} \rightarrow \mathbb{R}$,  e 
definida a seguinte equação,
\begin{equation}\label{eq:proof:theo:reglogrnr1nolinear:1}
y\equiv f_{\VECTOR{c}}(\VECTOR{x})= \frac{1}{1+e^{-h_{\VECTOR{c}}(\VECTOR{x}) }},
\end{equation}
ou seu equivalente: $logit(y)=h_{\VECTOR{c}}(\VECTOR{x})$,
onde $\VECTOR{c}=[c_1~ c_2~ ...~ c_m~ ...~ c_{M}]^{\transpose}$ e
$h_{\VECTOR{c}}(\VECTOR{x})\equiv h(\VECTOR{c},\VECTOR{x})$ é uma função de variável vetorial escolhida por nós.
Podemos definir um erro quadrático $e(\VECTOR{c})$ como
\begin{equation}\label{eq:proof:theo:reglogrnr1nolinear:2}
%e(\VECTOR{c}) = ||h(\VECTOR{x})-\VECTOR{y}||_{\MATRIX{W}}^2 \equiv \sum_{n=1}^{L} w_l||h(x_l)-y_l||^2,
e(\VECTOR{c}) =  
\sum_{n=1}^{L} w_l||h_{\VECTOR{c}}(\VECTOR{x}_l)-logit(y_l)||^2
+\alpha \sum_{n=1}^{M} d_m||c_m-c_{m,last}||^2;
\end{equation}
onde a função de custo $e(\VECTOR{c})$ toma em conta dois tipos de erro, separados em duas somatórias,
ponderadas mediante a variável $\alpha \in \mathbb{R}_+$,
a primeira somatória avalia o erro quadrático de $L$ amostras $\VECTOR{x}_l \in \mathbb{R}^{N}$ que pertencem a 
dois grupos etiquetados com a variável $y_l\in \{A,1-A\}$, 
onde $0<A\ll 0.5$ é escolhido por nós,
todos estes erros ponderados com os pesos $w_l \in \mathbb{R}_+$;
a segunda somatória de $e(\VECTOR{c})$ 
avalia as distancias dos valores $c_m$ ao redor de um valor $c_{m,last}$
previamente conhecido,
ponderadas com os pesos $d_m \in \mathbb{R}_+$.
Todos estes cálculos podem ser rescritos de forma matricial
na seguinte equação
\begin{equation}\label{eq:proof:theo:reglogrnr1nolinear:3}
e(\VECTOR{c}) \equiv ||\VECTOR{h}(\VECTOR{c})-\VECTOR{z}||_{\MATRIX{W}}^2
+ \alpha ||\VECTOR{c}-\VECTOR{c}_{last}||_{\MATRIX{D}}^2 ,
\end{equation}
onde
\begin{equation}\label{eq:proof:reglogrnr1nolinear:4}
\VECTOR{h}(\VECTOR{c})=\begin{bmatrix}
h(\VECTOR{c},\VECTOR{x}_1)\\ 
h(\VECTOR{c},\VECTOR{x}_2)\\ 
%\vdots\\ 
%h(\VECTOR{c},\VECTOR{x}_l)\\ 
\vdots\\ 
h(\VECTOR{c},\VECTOR{x}_L)
\end{bmatrix},
~
\VECTOR{z}=\begin{bmatrix}
logit(y_1)\\ 
logit(y_2)\\ 
%\vdots\\ 
%logit(y_l)\\ 
\vdots\\ 
logit(y_L)
\end{bmatrix},
~
\MATRIX{W}=\funcdiag\left(\begin{bmatrix}
w_1\\ 
w_2\\ 
%\vdots\\ 
%w_l\\ 
\vdots\\ 
w_L
\end{bmatrix}\right),
~
\MATRIX{D}=\funcdiag\left(\begin{bmatrix}
d_1\\ 
d_2\\ 
%\vdots\\ 
%d_m\\ 
\vdots\\ 
d_M
\end{bmatrix}\right).
\end{equation}
Usando o Teorema \ref{theo:minfxbCfxb}, sabemos que o vetor $\VECTOR{c}=\VECTOR{\hat{c}}$,
que minimiza a Eq. (\ref{eq:proof:theo:reglogrnr1nolinear:3}) pode ser achado usando iterativamente
\begin{equation}\label{eq:proof:theo:reglogrnr1nolinear:5}
\VECTOR{c}_{i}=\VECTOR{c}_{i-1}-[\MATRIX{J}(\VECTOR{c}_{i-1})^{\transpose}\MATRIX{W}\MATRIX{J}(\VECTOR{c}_{i-1})+\alpha \MATRIX{D}]^{-1}\MATRIX{J}(\VECTOR{c}_{i-1})^{\transpose}\MATRIX{W}[\VECTOR{h}(\VECTOR{c}_{i-1})-\VECTOR{z}],
\end{equation}
onde a matriz $\MATRIX{J}(\VECTOR{c})$ 
$\equiv \frac{\partial \VECTOR{h}(\VECTOR{c})}{\partial \VECTOR{c}^{\transpose}}$ é a 
\hyperref[def:jacobian]{\textbf{matriz Jacobiana}}  de $\VECTOR{h}(\VECTOR{c})$,
e $\VECTOR{c}_{last}\equiv \VECTOR{c}_{i-1}$,
\begin{equation}
\MATRIX{J}(\VECTOR{c})=\begin{bmatrix}
\VECTOR{j}(\VECTOR{c},\VECTOR{x}_1)\\ 
\VECTOR{j}(\VECTOR{c},\VECTOR{x}_2)\\ 
%\vdots\\ 
%\VECTOR{j}(\VECTOR{c},\VECTOR{x}_l)\\ 
\vdots\\ 
\VECTOR{j}(\VECTOR{c},\VECTOR{x}_L)
\end{bmatrix},
\quad
\begin{array}{lll}
\VECTOR{j}(\VECTOR{c},\VECTOR{x}) & = & \frac{\partial h(\VECTOR{c},\VECTOR{x})}{\partial \VECTOR{c}^{\transpose}} \\
                                ~ & ~ & ~\\
                                ~ & = & \left[\frac{\partial h(\VECTOR{c},\VECTOR{x})}{\partial c_1}\quad \frac{\partial h(\VECTOR{c},\VECTOR{x})}{\partial c_2}\quad ...\quad \frac{\partial h(\VECTOR{c},\VECTOR{x})}{\partial c_{m}} \quad ... \quad \frac{\partial h(\VECTOR{c},\VECTOR{x})}{\partial c_{M}} \right].
\end{array}
\end{equation}
\end{myproofT}

